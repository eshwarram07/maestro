<!--
Mission ID: b1beeea7-2b45-48b9-bd2c-10a119e5123a
OpenRouter Models Configured:
  Light: openai/gpt-4o-mini
  Heavy: google/gemini-2.5-flash-preview
  Beast: google/gemini-2.5-flash-preview
Stats:
  Total Cost: $0.522003
  Total Native Tokens: 2986697
  Total Web Searches: 0
Generated via: Streamlit UI
-->

# Algorithmic Resource Allocation: Interplay of Logics, Discretion, and Ethics on Inequality

# 1. Introduction

The increasing deployment of algorithmic decision systems (ADM) in public sector resource allocation contexts, such as healthcare, education, and social services, presents both opportunities for enhanced efficiency and transparency and significant challenges regarding equity and social justice. These systems do not operate in a vacuum; they interact dynamically with existing institutional logics, the exercise of professional discretion, and established ethical frameworks. This interaction is critical because it shapes how resources are distributed and can either reinforce or disrupt existing social inequalities. Understanding this complex interplay is essential for developing responsible and equitable ADM systems.

This report examines the literature concerning the interaction between algorithmic decision systems and their socio-institutional context in resource allocation. Specifically, it defines key concepts, including 'algorithmic decision systems' as computational processes designed to automate or assist human decision-making based on data inputs; 'institutional logics' as the sets of material practices and symbolic constructions, which constitute an organization's organizing principles and which are available to individuals and organizations to elaborate [10]; 'professional discretion' as the capacity of professionals to make independent judgments based on their expertise and context-specific knowledge; and 'ethical frameworks' as sets of principles and values used to guide moral reasoning and decision-making.

The significance of studying these interactions lies in their tangible impact on individuals and communities. When ADM systems are deployed in critical areas like healthcare access, educational opportunities, or social welfare provision, their design and implementation choices, influenced by institutional norms and the scope for professional judgment, can lead to biased outcomes, exacerbate disparities, and undermine trust in public institutions. Conversely, a thorough understanding of these dynamics can inform the development of systems and governance structures that promote fairness, accountability, and equitable distribution of resources.

This literature review is guided by the following research questions:
1. How do institutional logics influence the design and implementation of algorithmic decision systems in resource allocation contexts?
2. What is the role of professional discretion in mediating the outcomes of algorithmic decision systems in healthcare, education, and social services?
3. How do ethical frameworks inform the development and evaluation of algorithmic decision systems in resource allocation, particularly concerning fairness and equity?
4. How do the interactions between algorithmic decision systems, institutional logics, professional discretion, and ethical frameworks contribute to the reinforcement or disruption of existing social inequalities in resource allocation?

By addressing these questions, this report aims to synthesize current research, identify key challenges, and highlight areas for future inquiry regarding the responsible development and deployment of ADM systems in public sector resource allocation. The following section will delve into the existing literature on these topics.

# 2. Literature Review

Algorithmic decision systems (ADM) are increasingly deployed in public sector resource allocation contexts such as healthcare, education, and social services, aiming to enhance efficiency and transparency [4]. However, their implementation is complex, interacting with existing institutional logics, professional discretion, and ethical frameworks in ways that can reinforce or disrupt existing social inequalities. This literature review synthesizes research on these interactions, highlighting empirical examples and theoretical frameworks.

Institutional logics, defined as the formal and informal rules guiding decision-making within social contexts [9], significantly influence the design and implementation of ADM systems. In healthcare, for instance, competing logics such as managerialism (prioritizing efficiency and standardization), medical professionalism (centering clinician autonomy and patient care needs), and IT professionalism (emphasizing technical reliability and innovation) shape debates around IT governance [10]. These logics can lead to differing perspectives on fundamental IT dilemmas like centralization versus decentralization or standardization versus customization [10]. The enactment of these competing logics by stakeholders, such as hospital managers, clinicians, and IT staff, can result in polarization or compromise, impacting the effectiveness of IT systems [10]. This interplay of logics can influence the design choices of ADM systems, such as prioritizing efficiency in scheduling, which can then lead to unequal outcomes when implemented in a context marked by existing social disparities, thereby reinforcing social inequalities in access to timely care [5]. Similarly, in public administration, institutional structures and practices can lead to "organizational ignoring practices" that prevent institutions from addressing the detrimental consequences of ADM systems, even simple rule-based ones [6]. This can create "multilayered blackboxing," where the system's workings and unjust consequences become opaque and inaccessible, sustaining detrimental social and legal justice outcomes [6]. These ignoring practices represent a mechanism by which institutional inertia or strategic avoidance reinforces the negative distributional effects of ADM systems by preventing their identification and correction, thus perpetuating existing inequalities.

The role of professional discretion in the implementation and outcomes of algorithmic resource allocation is a critical area of inquiry. As decision-making shifts from humans to algorithms, there is a risk of losing critical thinking and domain expertise [4]. While ADM systems can augment human capabilities, the "automation paradox" suggests that automating simple tasks may leave humans with more complex oversight roles, potentially impacting motivation and requiring new skills [8]. Effective human-AI collaboration in professional services, which can be extended to resource allocation, requires acknowledging the capabilities of both humans and AI, cultivating a cooperative relationship, and integrating AI outputs into decision-making [12]. Practical strategies include dynamically transitioning between task automation and augmentation, leveraging AI for efficiency while allowing human judgment for complex cases [12]. Professional associations also play a role in recalibrating knowledge systems with AI, ensuring that AI-generated insights are validated and integrated into practice [12]. However, the influence of professional discretion can also be limited or constrained by the design and implementation of the ADM system itself or the institutional context [6]. For example, in a hospital ED staff allocation case study using a simulation-based optimization model, professional input from domain experts was used to filter potential solutions, demonstrating human discretion influencing algorithmic outcomes [11]. Conversely, systems designed to minimize human intervention can limit opportunities for professionals to mitigate bias or apply contextual knowledge, potentially leading to rigid and inequitable outcomes that reinforce disparities by removing the human capacity for context-aware adjustments.

Ethical considerations are paramount in algorithmic resource allocation. The increasing use of data, including sensitive data, raises concerns about data stewardship and the potential for misuse [8]. Ethical frameworks provide principles to guide decisions and behaviors to mitigate potential harm [8]. Algorithmic systems can pose threats to human dignity and autonomy, particularly when used in contexts like immigration enforcement, university admissions, or justice systems, where they have been shown to promote discrimination [4]. A sociomaterial perspective highlights the ethical interdependencies within human-AI systems, considering virtue and duty ethics at collective (social subsystem) and individual (sociomaterial practices) levels [2]. The way humans and AI interact—through substitution, augmentation, or assemblage—influences ethical dynamics [2]. Ethical challenges also arise in platform ecosystems that facilitate resource allocation, such as ridesharing or meal delivery, regarding the fair distribution of value created from data [8]. These ethical considerations should ideally inform the design principles of ADM systems to prevent outcomes that exacerbate social inequalities, though the translation of ethical principles into technical design and institutional practice remains a challenge, potentially leading to systems that, while technically efficient, fail to distribute resources equitably.

Algorithmic bias and discriminatory outcomes are significant concerns in resource allocation. Bias can stem from biased data, methodological issues, or the inclusion of protected variables or proxies, leading to direct or indirect discrimination [1]. Examples of algorithmic bias have been documented in various domains, including university admissions and hiring [1]. In medical appointment scheduling, for instance, predictive overbooking algorithms designed for efficiency can reinforce racial inequalities by disproportionately assigning patients from racial groups with higher no-show rates (often linked to socioeconomic obstacles) to less desirable appointment slots, resulting in longer waiting times for these groups [5]. This illustrates a direct mechanism: an algorithm optimized for a specific objective (efficiency), trained on data reflecting existing disparities (no-show rates correlated with race), produces outcomes (unequal waiting times) that reinforce those disparities in access to care. Transparency is often proposed as a mechanism to reveal potential discrimination, but its effectiveness depends on the type of bias; different transparency methods are needed to address direct versus indirect discrimination effectively [1]. Without effective mechanisms to identify and address bias, algorithmic systems risk perpetuating and even amplifying existing social inequalities in resource distribution.

The interactions between ADM systems and legal and institutional responses are crucial for addressing algorithmic injustice. When ADM systems produce errors or violations, public institutions, including administrative agencies and legal systems, may fail to address or correct these issues, thereby sustaining and reinforcing algorithmic injustices [6]. This can manifest as "organizational ignoring practices" [6] or "institutional blindness" [6], where institutions actively avoid knowing or addressing uncomfortable information [6]. The "Robodebt" case in Australia, where an automated welfare overpayment system produced invalid debts, illustrates how socially destructive ADM systems can arise and be sustained despite causing harm to vulnerable populations [4]. This case demonstrates how institutional and legal failures to adapt to algorithmic agency can prevent accountability and redress, allowing discriminatory outcomes to persist and reinforcing economic inequalities. Legal systems may be ill-equipped to restore social justice when algorithms err, sometimes avoiding addressing the core ADM issue itself [6]. This highlights the need for legal frameworks and institutional responses that specifically address the challenges posed by algorithmic agency and its consequences for social and legal justice [6], acting as a potential mechanism for disrupting the reinforcement of inequalities by providing avenues for challenge and correction.

Empirical examples across healthcare, education, and social services illustrate these dynamics. In healthcare, ADM systems are used for tasks like allocating preventive care [3], scheduling appointments [5], and managing IT governance [10]. A data-driven model for allocating preventive care for Type II Diabetes Mellitus demonstrates how algorithms can optimize resource allocation based on comprehensive data, potentially improving outcomes compared to traditional clinical methods [3]. However, as seen in the appointment scheduling example, efficiency optimization can inadvertently lead to unequal outcomes based on factors correlated with social inequalities [5], directly impacting equitable access to care. In education, ADM systems for school placements have been shown to produce biased outcomes and highlight how institutional responses can fail to correct these injustices [6], reinforcing educational disparities. Social services have also seen problematic implementations, such as systems for detecting welfare overpayments that have disproportionately harmed vulnerable populations [4], exacerbating economic insecurity. These cases collectively demonstrate how the design, implementation, and institutional context of ADM systems in resource allocation interact to produce distributional outcomes that can reinforce existing social inequalities, often through mechanisms involving the embedding of institutional logics in algorithmic objectives, the constraint or enablement of professional discretion, and the failure of institutional and legal systems to ensure accountability and address bias.

The following section will delve deeper into the theoretical discussion arising from these empirical findings and conceptual frameworks.

# 3. Discussion

The preceding literature review has established that algorithmic decision systems (ADM) in resource allocation contexts operate within a complex ecosystem shaped by institutional logics, professional discretion, and ethical frameworks. The empirical evidence highlights that far from being neutral tools, these systems can significantly influence distributional outcomes, often reinforcing existing social inequalities. This discussion critically examines the interplay of these factors, drawing on the reviewed literature to illuminate the mechanisms through which ADM systems produce these effects, and identifying key areas for future research.

The interaction between ADM systems and prevailing institutional logics is fundamental to understanding their impact on resource allocation. Institutional logics, as organizing principles [10], shape what is considered legitimate and effective within a given sector. In healthcare, for instance, the tension between managerial logics prioritizing efficiency and standardization, medical professionalism focused on patient-centered care and clinician autonomy, and IT professionalism emphasizing technical reliability influences the design and adoption of IT systems, including those with algorithmic components [10]. These competing logics can lead to dilemmas in IT governance, such as debates over centralized versus decentralized control [10]. The dominance of one logic, such as a focus on efficiency in appointment scheduling, can inadvertently lead to outcomes that disadvantage certain groups, as seen in the case where predictive overbooking algorithms, driven by a cost-minimization objective informed by a managerial logic, resulted in longer waiting times for racial groups with higher predicted no-show rates [5]. This demonstrates a key mechanism: an institutional logic (efficiency) embedded in the algorithmic objective function, when combined with data reflecting existing social inequalities, produces inequitable distributional outcomes by prioritizing system performance over equitable patient experience. Similarly, in public administration, institutional practices of "organizational ignoring" [6] can prevent institutions from addressing the detrimental consequences of ADM systems, contributing to "multilayered blackboxing" [6] and perpetuating social and legal injustice, even with technically simple algorithms [6]. This represents a mechanism of institutional inertia and strategic avoidance that shields the ADM's problematic design or implementation from scrutiny, allowing detrimental distributional outcomes to persist.

Case studies provide concrete examples of how algorithmic systems, embedded within institutional contexts, can lead to unjust outcomes. The use of an ADM system for public school placements in Gothenburg, Sweden, intended to improve efficiency and equal treatment, instead resulted in thousands of erroneous placements that violated national law [6]. This outcome was not merely a technical error but was exacerbated by the institutional response, where the Public School Administration and the court system engaged in ignoring practices, failing to address the systematic errors and contributing to institutional blackboxing [6]. This case illustrates the mechanism by which institutional arrangements and a failure to adapt legal and procedural frameworks to the reality of algorithmic decision-making can shield algorithms from scrutiny and remedy, reinforcing social and legal injustice by preventing the correction of biased resource allocation (school placements). Another prominent example is the Australian "Robodebt" system, an automated welfare debt recovery program [4]. Driven by a welfare-critical ideology and economic imperative, this system minimized human agency and maximized algorithmic agency, leading to discrimination at scale and causing significant harm to vulnerable citizens [4]. The institutional failure to acknowledge and address these destructive consequences, influenced by a fixed ideology and unwillingness to admit error, prolonged the harm [4]. Here, an institutional logic (punitive welfare approach) directly shaped the ADM design (minimal human oversight, automated debt calculation), leading to discriminatory distributional outcomes (incorrect debts disproportionately impacting vulnerable populations) by employing the mechanism of removing professional discretion to intervene. These cases underscore that algorithmic injustice is not solely a technical problem but is deeply intertwined with institutional practices, power dynamics, and the failure of governance mechanisms that mediate the algorithm's impact.

Professional discretion plays a complex and evolving role in the context of algorithmic resource allocation. While ADM systems can automate tasks and augment human capabilities, they also necessitate a re-evaluation of professional roles [12]. Professionals may transition from task executors to monitors and investigators of AI outcomes [12], requiring new skills and potentially leading to anxiety or degraded performance if not managed effectively [15]. Integrating AI into professional decision-making, such as in diagnosis or inference, requires balancing AI's formal rationality with human substantive rationality, acknowledging AI's potential lack of context or biased judgment [12]. However, the extent to which professional discretion can influence algorithmic outcomes and mitigate negative distributional effects is often constrained by the system's design and the institutional context [6]. In the Robodebt case, the minimization of human agency in favor of algorithmic processing directly limited the potential for professional discretion to mitigate harmful outcomes [4]. This design choice, influenced by institutional imperatives, directly contributed to the discriminatory impact by employing the mechanism of constraining professional judgment and preventing case-by-case human review. Conversely, in some healthcare IT governance scenarios, professional logics (like medical or IT professionalism) can potentially synthesize or bridge contradictions between competing logics, suggesting a potential, though not always realized, mechanism for professional discretion in achieving more balanced outcomes [10]. Research highlights the need for organizations to devise policies and procedures for combining human judgment and algorithmic decision-making, and to understand how contextual factors influence the balance of control [15]. The degree to which professional discretion is enabled or constrained by ADM design and institutional norms is a key mechanism influencing whether the system reinforces or disrupts existing inequalities.

Ethical frameworks are essential for navigating the challenges posed by algorithmic resource allocation, particularly concerning fairness and accountability. Ethical principles guide decisions and behaviors to mitigate harm [8]. However, applying traditional ethical frameworks, such as utilitarianism, which aims to maximize the greater good, can conflict with principles of Diversity, Equity, and Inclusion (DEI) when applied to algorithmic systems [7]. A utilitarian approach might prioritize overall efficiency or aggregate health gains, potentially leading to outcomes that disadvantage minorities or those at the margin, who are specifically addressed by DEI principles [7][14]. The healthcare resource allocation model illustrating the trade-offs between utilitarian and egalitarian objectives demonstrates this tension; maximizing aggregate health may lead to different allocation strategies than prioritizing the health of the worst off [14]. This highlights the mechanism by which different ethical objectives, when operationalized in algorithmic design, lead to distinct distributional outcomes. Furthermore, algorithmic bias, stemming from biased data or design, can lead to direct or indirect discrimination [1], resulting in unequal distribution of benefits or burdens and negatively impacting fairness perceptions [1]. The lack of diversity in design teams can contribute to algorithms reflecting the biases of their creators, disproportionately impacting marginalized groups [7]. Accountability for the consequences of algorithmic decisions remains a significant challenge [15], particularly when systems are opaque ("black boxes") [1]. While transparency is proposed as a means to reveal bias and support legal compliance [1], its effectiveness is debated and depends on the type of bias and the transparency method used [1]. The Robodebt case highlighted critical deficiencies in legal frameworks and oversight regarding government ADM use, underscoring the need for robust governance and accountability mechanisms [4]. The application and interpretation of ethical frameworks thus directly influence ADM design choices and the institutional responses to algorithmic outcomes, shaping their impact on inequality.

Despite growing research, significant gaps remain in fully understanding and addressing the complex interactions between ADM, institutional logics, professional discretion, and ethical frameworks in resource allocation, particularly concerning social inequalities. Key areas for future research include:

*   Explicitly detailing the specific mechanisms by which different institutional logics shape ADM design choices (e.g., objective functions, data inclusion, level of automation) and how these design choices directly lead to the reinforcement or disruption of social inequalities in healthcare, education, and social services.
*   Investigating how professional discretion mediates the implementation and outcomes of ADM systems in practice, including the ability of professionals to override, interpret, or challenge algorithmic outputs, and how variations in this ability affect distributional outcomes.
*   Empirically examining how human work tasks and skill requirements change with ADM implementation and how workers respond, with a focus on how these changes impact their capacity for equitable decision-making and the exercise of professional judgment.
*   Studying the dynamics of combining human judgment and AI for decision-making, including the contextual factors influencing the balance of control, and how different balances affect distributional outcomes and the potential for bias mitigation.
*   Exploring how accountability for ADM applications is managed in practice, including legal liability and the development of effective mechanisms for redress when algorithmic decisions perpetuate inequality.
*   Researching the characteristics of 'responsible' ADM systems and developing methods for identifying bias in emerging capabilities, directly linking these technical and design considerations to the potential for reinforcing or disrupting social inequalities.
*   Investigating how global companies navigate inconsistent local laws regarding automated systems and workforce fairness, exploring how these legal and institutional variations influence the design and impact of ADM on inequality across different contexts.
*   Employing interdisciplinary and mixed-methods approaches, potentially using action design research, to guide towards synthesis among competing logics in the development and governance of algorithmic systems [10][15].
*   Examining treatment intensity as a mediator in healthcare disparities based on insurance type [13] and incorporating social determinants of health into algorithmic models [14] as specific avenues to address equity concerns in healthcare resource allocation by investigating the interplay of institutional factors (financial incentives), algorithmic design (model inputs), and professional practice (treatment intensity) in producing unequal outcomes.

The discussion has highlighted how algorithmic decision systems interact with institutional logics, professional discretion, and ethical frameworks to shape resource allocation outcomes, often reinforcing social inequalities. The case studies demonstrate the tangible negative impacts of poorly designed or contextually inappropriate ADM systems, particularly when coupled with institutional failures to ensure accountability and justice. Addressing these challenges requires not only technical solutions but also a critical examination of the institutional contexts, the role of human agency, and the ethical principles guiding the design and implementation of these powerful systems. The following section will synthesize these findings into a comprehensive conclusion, offering policy implications and a roadmap for future research.

# 4. Conclusion

This report has explored the complex interactions between algorithmic decision systems (ADM) used in resource allocation contexts (healthcare, education, social services) and the existing institutional logics, professional discretion, and ethical frameworks that shape their implementation and outcomes. The literature review and subsequent discussion reveal that ADM systems are not neutral tools but are deeply embedded within socio-technical systems, where their design, deployment, and consequences are mediated by broader organizational, professional, and ethical considerations.

A key finding is that prevailing institutional logics significantly influence how ADM systems are conceived and utilized. Competing logics, such as efficiency-driven managerialism versus patient-centred medical professionalism, create tensions that shape IT governance and can lead to algorithmic designs that inadvertently prioritize certain outcomes (e.g., efficiency) over others (e.g., equity) [5][10]. Furthermore, institutional practices, such as "organizational ignoring" [6], can prevent institutions from acknowledging and rectifying the detrimental consequences of ADM systems, perpetuating algorithmic injustices [6]. Case studies, such as the Gothenburg school placement errors and the Australian Robodebt system, vividly illustrate how institutional failures to adapt governance and legal frameworks to algorithmic agency can shield systems from scrutiny and accountability, leading to significant harm to vulnerable populations [4][6].

Professional discretion emerges as a critical, yet often constrained, factor. While ADM systems can augment human capabilities, they also shift the nature of professional work, requiring new skills and potentially limiting the scope for human judgment [12][15]. The extent to which professionals can mitigate algorithmic bias or challenge problematic outcomes is often dependent on the system's design and the institutional context [6]. Minimizing human agency in favor of algorithmic processing, as seen in the Robodebt case, can directly impede the potential for professional intervention to prevent harm [4].

Ethical considerations, particularly regarding fairness, equity, and accountability, are paramount. The tension between maximizing aggregate welfare (utilitarianism) and ensuring equitable outcomes for marginalized groups (DEI principles) highlights a fundamental challenge in designing ethical ADM systems for resource allocation [7][14]. Algorithmic bias, stemming from data or design flaws, can lead to discriminatory outcomes, reinforcing existing social inequalities [1]. Addressing these issues requires moving beyond simple transparency and developing robust governance structures that ensure accountability and provide mechanisms for redress when algorithms err [1][4].

The implications for policy and practice are significant. Policymakers and practitioners must recognize that implementing ADM systems in resource allocation is not merely a technical exercise but a socio-technical intervention with profound social consequences. Recommendations for addressing social inequalities in algorithmic decision-making, directly linked to the identified interactions and mechanisms, include:

1.  **Critical Examination of Institutional Logics and their Impact on Design:** Actively identify and critically evaluate the dominant institutional logics shaping ADM system design (e.g., objective functions, data selection, level of automation). Promote dialogue and potential synthesis among competing logics to ensure that equity and fairness are explicitly prioritized alongside efficiency, thus directly influencing the mechanisms that can reinforce or disrupt unequal outcomes [10].
2.  **Empowering and Integrating Professional Discretion:** Design ADM systems and institutional processes that facilitate meaningful professional oversight and the integration of human judgment, particularly in complex or sensitive cases where contextual knowledge is crucial. Providing training and support enables professionals to effectively interact with and evaluate algorithmic outputs, acting as a mechanism to potentially mitigate bias and adapt decisions for equitable outcomes [12][15].
3.  **Robust Ethical Frameworks and Accountable Governance Structures:** Develop and implement comprehensive ethical frameworks that explicitly address issues of fairness, equity, and accountability in algorithmic decision-making in resource allocation contexts. Establish clear governance structures, including legal and institutional mechanisms, to ensure accountability for algorithmic outcomes and provide accessible pathways for redress when algorithms err, counteracting "organizational ignoring" and facilitating the correction of injustices [2][4][15].
4.  **Systematic Identification and Mitigation of Algorithmic Bias:** Implement strategies to identify and mitigate algorithmic bias throughout the system lifecycle, from data collection to model deployment and monitoring. This includes diversifying design teams, critically evaluating data sources for embedded disparities, and incorporating social determinants of health or other relevant contextual factors into models where appropriate to directly address mechanisms leading to discriminatory outcomes and reinforce social inequalities [7][1][14].
5.  **Continuous Learning from Empirical Cases and Contexts:** Continuously study and learn from empirical examples of ADM implementation in resource allocation, both successes and failures, across diverse institutional settings. Understanding the complex interplay of technical, institutional, professional, and ethical factors in specific contexts informs best practices and helps identify mechanisms that either perpetuate or challenge existing inequalities [4][6].

Addressing the challenges of algorithmic decision systems in resource allocation requires a holistic approach that considers the intricate relationships between technology and its social, institutional, and ethical context. Future research should continue to explore these dynamics through interdisciplinary and empirical studies, focusing on specific mechanisms of interaction and developing practical strategies for designing and governing ADM systems that promote equitable and just outcomes.

## References

1. Sepideh Ebrahimi, Esraa Abdelhalim, Khaled Hassanein, Milena Head. (2024). Reducing the incidence of biased algorithmic decisions through feature importance transparency: an empirical study. *European Journal of Information Systems*.
2. Teresa Heyder, Nina Passlack, Oliver Posegga. (2023). Ethical management of human-AI interaction: Theory development review. *Journal of Strategic Information Systems*.
3. Mathias Kraus, Stefan Feuerriegel, Maytal Saar-Tsechansky. (2024). Data-Driven Allocation of Preventive Care with Application to Diabetes Mellitus Type II. *Manufacturing & Service Operations Management*.
4. Tapani Rinta-Kahila, Ida Someh, Nicole Gillespie, Marta Indulska, Shirley Gregor. (2022). Algorithmic decision-making and system destructiveness: A case of automatic debt recovery. *European Journal of Information Systems*.
5. Michele Samorani, Shannon L. Harris, Linda Goler Blount, Haibing Lu, Michael A. Santoro. (2022). Overbooked and Overlooked: Machine Learning and Racial Bias in Medical Appointment Scheduling. *Manufacturing & Service Operations Management*.
6. Charlotta Kronblad, Anna Essén, Magnus Mähring. (2024). When Justice is Blind to Algorithms: Multilayered Blackboxing of Algorithmic Decision Making in the Public Sector.
7. (2024). The strategic value of DEI in the information systems discipline. *Journal of Strategic Information Systems*.
8. (2023). Digital transformation in operations management: Fundamental change through agency reversal. *Journal of Operations Management*.
9. William Diebel, Jury Gualandris, Robert D. Klassen. (2024). How do suppliers respond to institutional complexity? Examining voluntary public environmental disclosure in a global manufacturing supply network. *Journal of Operations Management*.
10. Albert Boonstra, U. Yeliz Eseryel, Marjolein A. G. van Offenbeek. (2018). Stakeholders’ enactment of competing logics in IT governance: polarization, compromise or synthesis?. *European Journal of Information Systems*.
11. Weiwei Chen, Siyang Gao, Wenjie Chen, Jianzhong Du. (2023). Optimizing resource allocation in service systems via simulation: A Bayesian formulation. *Production and Operations Management*.
12. Jiaqi Yang, Alireza Amrollahi, Mauricio Marrone. (2024). Harnessing the Potential of Artificial Intelligence: Affordances, Constraints, and Strategic Implications for Professional Services. *Journal of Strategic Information Systems*.
13. Luv Sharma, Deepa Goradia. (2023). Will your insurance type influence clinical quality outcomes? An investigation of contributing factors, underlying mechanism, and consequences. *Production and Operations Management*.
14. Yrjänä Hynninen, Eeva Vilkkumaa, Ahti Salo. (2021). Operationalization of Utilitarian and Egalitarian Objectives for Optimal Allocation of Health Care Resources. *Decision Sciences*.
15. Crispin Coombs, Donald Hislop, Stanimira K. Taneva, Sarah Barnard. (2020). The strategic impacts of Intelligent Automation for knowledge and service work: An interdisciplinary review. *Journal of Strategic Information Systems*.